202212061411
Status: #idea 
Tags: [[Machine Learning]]

# Learning Rate Decay

Pensando na iteração que acontece em cada [[Gradient Descent]] para movimentar o erro a partir de um learning rate, muitas vezes entramos no caso em que grandes passos são vantajosos no começo, mas isso pode dificultar com que consigam convergir no mínimo local. 
Com isso em vista, existem várias forma de criar um cálculo do Learning Rate utilizado para cada passo. De forma que seu valor diminua quanto mais avançar nas epochs.

---
# References
https://www.coursera.org/learn/deep-neural-network/lecture/hjgIA/learning-rate-decay